---
title: Auto-Scaling in OpenFaas
date: 2019-06-15 20:01:22
tags: OpenFaas
---

## Auto-Scaling in OpenFaas

In this post, what I am focusing on is auto-scaling in OpenFaas, which allows a function to scale up or down depending on demand represented by different metrics. 

But, before taking about auto-scaling,  let's start with the design and architecture of OpenFaas.

### Design

Here I present the conceptual architecture of OpenFaas. However, the orchestration provider I use is Swarm rather than Kubernetes. According to the image, my OpenFaas consists of four parts: **Gateway**, **Watchdog**, **Prometheus** and **Swarm** and I will make an introdution for them below.

![faas](/Users/zengguangtao/Desktop/faas.jpeg)



#### Gateway

Gateway provides an external route into your functions and collects Cloud Native metrics through Prometheus. **In a word, the Gateway, is just a door for user to access to the api function and provide matrics for Prometheus.**

![of-conceptual-operator](/Users/zengguangtao/Desktop/of-conceptual-operator.png)

 In addition, it offer *a build-in UI portal* for user to deploy function either by you own or from the Function Store. What's more, the connection between **Gateway** and **Prometheus** is the key to make auto-scaling, which means that can make auto-scaling via AlertManager and Prometheus.



#### WatchDog

According to the official document, the watchdog provides an unmanaged and generic interface between the outside world and your function. Its job is to marshal a HTTP request accepted on the API Gateway and to invoke your chosen application. The diagram below indicates how Watchdog works as a web server.

![68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f4447536344626c554941416f34482d2e6a70673a6c61726765](/Users/zengguangtao/Desktop/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f4447536344626c554941416f34482d2e6a70673a6c61726765.jpeg)

You can consider Watchdog as a entry for your fuction that is deloyed in docker.



#### Prometheus

[Prometheus](https://github.com/prometheus) is an open-source systems monitoring and alerting toolkit originally built at SoundCloud. In OpenFaas, Prometheus help Gateway *collect Cloud Native metrics* and *fire an alert to the API Gateway* to inform Gateway to scale.

![prometheus](/Users/zengguangtao/Desktop/prometheus.png)



#### Swarm

A swarm is a group of machines that are running Docker and joined into a cluster. In OpenFass, swarm is used to create cluster in which you can easily to set up your docker function in a short period of time. Read [Swarm](https://docs.docker.com/engine/swarm/) for more details.

![swarm](/Users/zengguangtao/Desktop/swarm.png)



### How OpenFaas Scale

The gateway will scale functions according to demand by altering the service replica count in the Docker Swarm or Kubernetes API. Custom alerts generated by AlertManager are received on the `/system/alert `endpoint.

Let's figure out the mechanism why OpenFaas can make auto-scaling.

First, in auto-scaling, prometheus plays a important role. Check the config of prometheus first.

##### alert.rules.yml

```yml
groups:
- name: prometheus/alert.rules
  rules:
  - alert: service_down
    expr: up == 0
  - alert: APIHighInvocationRate
    expr: sum(rate(gateway_function_invocation_total{code="200"}[10s])) BY (function_name) > 5
    for: 5s
    labels:
      service: gateway
      severity: major
    annotations:
      description: High invocation total on {{ $labels.function_name }}
      summary: High invocation total on {{ $labels.function_name }}
```

In the *yml* file, we can know that there is an alert named APIHighInvocationRate, which will sent a file alert to API Gateway when `sum(rate(gateway_function_invocation_total{code="200"}[10s])) BY (function_name) > 5`(whether a function was called for at least 5 times in last 10 seconds). But only the alert lasts for 5s, can Gateway scale the function.

Continuing to focus on the config of alertmanager.

##### alertmanager.yml

```yaml
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 5s
  group_interval: 10s
  repeat_interval: 30s
  receiver: scale-up
  routes:
  - match:
      service: gateway
      receiver: scale-up
      severity: major
inhibit_rules:
- source_match:
    severity: 'critical'
  target_match:
    severity: 'warning'
  equal: ['alertname', 'cluster', 'service']
receivers:
- name: 'scale-up'
  webhook_configs:
    - url: http://gateway:8080/system/alert
      send_resolved: true
      http_config:
            basic_auth:
              username: admin
              password_file: /run/secrets/basic-auth-password
```

Yep! We find the reason why OpenFaas can auto scale the function. Prometheus will check the number each function calls intermittently. When it find that APIHighInvocationRate appear and last for 5s, it will alert and send a command to `http://gateway:8080/system/alert` to scale up.



Curiously, I trun to the `system/alert` api interface and show the core code below. You can get more detail in [alerthandler](https://github.com/openfaas/faas/blob/master/gateway/handlers/alerthandler.go).

```go
// MakeAlertHandler handles alerts from Prometheus Alertmanager
func MakeAlertHandler(service scaling.ServiceQuery) http.HandlerFunc {
	return func(w http.ResponseWriter, r *http.Request) {

		log.Println("Alert received.")

		body, readErr := ioutil.ReadAll(r.Body)

		log.Println(string(body))

		if readErr != nil {
			w.WriteHeader(http.StatusBadRequest)
			w.Write([]byte("Unable to read alert."))

			log.Println(readErr)
			return
		}

		var req requests.PrometheusAlert
		err := json.Unmarshal(body, &req)
		if err != nil {
			w.WriteHeader(http.StatusBadRequest)
			w.Write([]byte("Unable to parse alert, bad format."))
			log.Println(err)
			return
		}

		errors := handleAlerts(&req, service)
		if len(errors) > 0 {
			log.Println(errors)
			var errorOutput string
			for d, err := range errors {
				errorOutput += fmt.Sprintf("[%d] %s\n", d, err)
			}
			w.WriteHeader(http.StatusInternalServerError)
			w.Write([]byte(errorOutput))
			return
		}

		w.WriteHeader(http.StatusOK)
	}
}

func handleAlerts(req *requests.PrometheusAlert, service scaling.ServiceQuery) []error {
	var errors []error
	for _, alert := range req.Alerts {
		if err := scaleService(alert, service); err != nil {
			log.Println(err)
			errors = append(errors, err)
		}
	}

	return errors
}
```

In the api, Gateway will get the information from AlertManager, decide what replica count to set and scale.



### Monitor Auto-scaling in Open-Faas UI

> We will use ***play_with_docker*** to implement the tutorial.

First, deploy the `echo-fn` function in OpenFaas and set the scale-config.

```shell
$ git clone https://github.com/alexellis/echo-fn \
 && cd echo-fn \
 && faas-cli template store pull golang-http \
 && faas-cli deploy \
  --label com.openfaas.scale.max=10 \
  --label com.openfaas.scale.min=1
```

Then, you can get a function echo-fn in OpenFaas UI. 

![UI](/Users/zengguangtao/Desktop/UI.png)

Get into the alerts UI portal in [192.168.0.12:9090/alerts](192.168.0.12:9090/alerts). However, there is no alert now.

![alert](/Users/zengguangtao/Desktop/alert.png)



Use shell command to invoke the `go-echo` function over and over until seeing the replica count scale. 

```shell
$ for i in {0..10000};
do
   echo -n "Post $i" | faas-cli invoke go-echo && echo;
done;
```



After run the command and  `go-echo` is invoked again and again, AlertManager will find that `sum(rate(gateway_function_invocation_total{code="200"}[10s])) BY (function_name) > 5`  becomes `True`. Than it will be in **PENDING** state and continue to monitor the alert for 5s.

![pending](/Users/zengguangtao/Desktop/pending.png)

If the alert lasts for 5s, AlertManager will switch to **FIRING** state and sent to `/system/alert ` endpoint to scale the function.

![firing](/Users/zengguangtao/Desktop/firing.png)

After a few second, you can find the replica number wil be scaled to 5.

![scale-replica](/Users/zengguangtao/Desktop/scale-replica.png)

When you stop the command, Gateway will figure out that some replica get to be idle and kill them. After that, the replica number will down back to 1.

